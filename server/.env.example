NODE_ENV=development

# Db connection info.
# Providing a single connection url is more common, but Sequelize requires the
# individual component values, and AWS RDS secrets expose each component
# separately by default, so we might as well have a separate variable for each.
DATABASE_HOST=127.0.0.1
DATABASE_READ_ONLY_HOST=127.0.0.1
DATABASE_PORT=5432
DATABASE_NAME=postgres
DATABASE_USER=postgres
DATABASE_PASSWORD=postgres123

# Used to sign session ids in cookies.
SESSION_SECRET=some_long_secret_string

# Warehouse / Analytics Provider Default will be clickhouse
WAREHOUSE_ADAPTER=clickhouse
ANALYTICS_ADAPTER=clickhouse

# ClickHouse Configuration
CLICKHOUSE_HOST=localhost
CLICKHOUSE_PORT=8123
CLICKHOUSE_USERNAME=default
CLICKHOUSE_PASSWORD=clickhouse
CLICKHOUSE_DATABASE=analytics
CLICKHOUSE_PROTOCOL=http

# Snowflake Personal Account Credentials
SNOWFLAKE_USERNAME=
SNOWFLAKE_PASSWORD=
SNOWFLAKE_DB_NAME=MY_DEV_DB
SNOWFLAKE_ACCOUNT=

HMA_SERVICE_URL=http://localhost:5000

# Kafka authentication info.
KAFKA_BROKER_HOST=localhost:29092
KAFKA_BROKER_USERNAME=
KAFKA_BROKER_PASSWORD=
KAFKA_SCHEMA_REGISTRY_HOST=http://localhost:8081
KAFKA_SCHEMA_REGISTRY_USERNAME=
KAFKA_SCHEMA_REGISTRY_PASSWORD=
KAFKAJS_NO_PARTITIONER_WARNING=1

# NB: these schema ids are different on prod + staging clusters; may be
# different in our future local dev setup. Eventually, we'll likely want
# a more sophisticated approach than env vars for determining these values,
# but we need to figure out our Kafka schema migration system first.
KAFKA_TOPIC_KEY_SCHEMA_ID_SNOWFLAKE_INGEST_EVENTS=
KAFKA_TOPIC_KEY_SCHEMA_ID_DATA_WAREHOUSE_INGEST_EVENTS=
KAFKA_TOPIC_VALUE_SCHEMA_ID_SNOWFLAKE_INGEST_EVENTS=
KAFKA_TOPIC_VALUE_SCHEMA_ID_DATA_WAREHOUSE_INGEST_EVENTS=
KAFKA_TOPIC_KEY_SCHEMA_ID_ITEM_SUBMISSION_EVENTS=
KAFKA_TOPIC_VALUE_SCHEMA_ID_ITEM_SUBMISSION_EVENTS=
KAFKA_TOPIC_KEY_SCHEMA_ID_ITEM_SUBMISSION_EVENTS_RETRY_0=
KAFKA_TOPIC_VALUE_SCHEMA_ID_ITEM_SUBMISSION_EVENTS_RETRY_0=

# Bucket info for moving pg data in bulk into Snowflake
SNOWFLAKE_S3_BUCKET_NAME=
SNOWFLAKE_S3_BUCKET_REGION=us-east-2

# Scylla Cluster Details
SCYLLA_USERNAME=cassandra
SCYLLA_PASSWORD=cassandra
SCYLLA_HOSTS='127.0.0.1:9042'
SCYLLA_LOCAL_DATACENTER='datacenter1'

# Local dev
NODE_ENV=development
EXPOSE_SENSITIVE_IMPLEMENTATION_DETAILS_IN_ERRORS=true
ALLOW_USER_INPUT_LOCALHOST_URIS=true
SEQUELIZE_PRINT_LOGS=true
SNOWFLAKE_PRINT_LOGS=true

# Redis Credentials
REDIS_USE_CLUSTER=false
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_USER=
REDIS_PASSWORD=

# Signal API Keys/URLs
OPEN_AI_API_KEY=

OTEL_SERVICE_NAME=COOP_TEST_SERVICE

# Email Configuration
NOREPLY_EMAIL=noreply@example.com
SUPPORT_EMAIL=support@example.com
TEAM_EMAIL=team@example.com

# Other API Keys
SENDGRID_API_KEY=SG.REAL_API_KEY_HERE
GOOGLE_PLACES_API_KEY=
READ_ME_JWT_SECRET=
LAUNCHDARKLY_SECRET=
GOOGLE_TRANSLATE_API_KEY=
# Secret used to sign JWTs that are generated when encoding OpaqueScalar values
# in GraphQL resolvers.
GRAPHQL_OPAQUE_SCALAR_SECRET=some_long_secret_string

# AWS API Gateway Usage Plan IDs
# These usage plan IDs are injected as environment variables by the CDK code. We need
# to have a usage plan ID that we can test with locally, and it should have the same
# name as the prod-equivalent usage plan. When we deploy new usage plans, we need
# to add a corresponding env var here with the same name. The format should be
# uppercase with underscores between words, and the suffix is always _API_USAGE_PLAN_ID.

LOG_REQUEST_BODY=true

SLACK_APP_BEARER_TOKEN=
GROQ_SECRET_KEY=

ITEM_QUEUE_TRAFFIC_PERCENTAGE='0.05'
UI_URL=http://localhost:3000
